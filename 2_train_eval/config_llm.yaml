# Common configuration for all models
prompt_type: "zero_shot"
seed: 42
mode: "inference" 

data_file: "data/survey_questions.json"
logs_dir: "results"

# Configuration per model
# TODO: check params they're random placeholder
models:  # List of models to evaluate
  - model: "gemini-2.5-pro"
    temperature: 0.8
    sc_runs: 1
    use_rate_limiter: true
    requests_per_second: 0.8
    check_every_n_seconds: 1.5
    batched: true
    debug: false
    num_samples: 10
  
  - model: "gemini-2.0-flash"
    temperature: 0.7
    sc_runs: 2
    use_rate_limiter: true
    requests_per_second: 1.0
    check_every_n_seconds: 1.0
    batched: true
    debug: false
    num_samples: 10