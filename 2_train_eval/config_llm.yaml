# Common configuration for all models
debug: true
prompt_type: "zero_shot"
seed: 42
mode: "inference" 
data_file: "1_data/converted_questions.json"
logs_dir: "results"

# Configuration per model
# TODO: check params they're random placeholder
models:  # List of models to evaluate
  - model: "gpt-4o-mini"
    temperature: 0.8
    sc_runs: 1
    use_rate_limiter: true
    requests_per_second: 0.8
    check_every_n_seconds: 1.5
    batched: true
    num_samples: 3
  
  # Local model example
  - model: "mistral-7b"
    local: "mistralai/Mistral-7B-Instruct-v0.2"
    temperature: 0.7
    sc_runs: 1
    use_rate_limiter: false
    batched: true
    num_samples: 3
  
  # - model: "gemini-2.0-flash"
  #   temperature: 0.7
  #   sc_runs: 2
  #   use_rate_limiter: true
  #   requests_per_second: 1.0
  #   check_every_n_seconds: 1.0
  #   batched: true
  #   debug: false
  #   num_samples: 10